---
title: 'Inflation Forecast'
author: "Owen Ragsdale"
date: "`r format(Sys.Date(),'%B %d, %Y')`"
output: 
  html_document:
    df_print: paged
    code_folding: "hide"
    toc: yes
    fig_caption: yes
    theme: darkly
    toc_float: no
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
graphics.off()

```
```{r load packages, include=FALSE}
require(fpp3)
require(tsibble)
require(tidyverse)
require(tidyquant)
require(lubridate)
require(timetk)
require(kableExtra)
require(reshape2)

```

```{r data, include=FALSE}

varList <- c("PCEPI", "UNRATE", "MICH", "IPMAN", "HOUST")
X <-
  tq_get(varList, get = "economic.data", from = "1982-01-01") %>%
  mutate(Month = yearmonth(date), value = price) %>%
  dplyr::select(-c(date, price)) %>%
  as_tsibble(index = Month, key = symbol)
Xw <- X %>%
  pivot_wider(names_from = symbol, values_from = value) %>%
  as_tsibble()  %>% 
  drop_na()

```

### _Executive Summary_
This paper outlines the development and testing of forecast models derived from the Phillips curve. Several variables are tested individually and collectively to see if they are able to better forecast the rate of inflation, rather than the traditional variable of unemployment which has been historically used in Phillips curves. After variable identification and manipulation the results would suggest that _IPMAN_ , the variable used to represent Industrial Production in manufacturing made the strongest case that it was the most capable to predict the rate of inflation. However, all of our models including the ensemble (combination of all the models) were not particularly effective in predicting inflation. 


```{r unit root, include=FALSE}
Xw <- Xw %>%
  mutate(dif_MICH = difference(MICH))

Xw %>%
  features(dif_MICH, unitroot_kpss)



```


### _Introduction_

Over the last several decades inflation has become a metric of concern among economist across the globe. During this time, economist have tried to build several different types of models that include a wide variety of variables to forecast and pin down co-variance's and correlations between inflation and other key metrics within the economy. The Phillips curve has been a effective model for predicting the rate of inflation, using the rate of inflation on the Y-axis and the rate of unemployment on the X-axis. This paper will explore the efficacy of replacing unemployment with other variables by using statistical test to determine whether variables other than unemployment rate can accurately forecast the rate of inflation.

To view the paper and code please click the link below.
[link](https://owenrags.github.io/)

### _Data selection & manipulation_

As mentioned in the previous paragraph the goal of this paper is to test the efficacy of replacing unemployment in the Phillips curve with other variables. The other variables that will be tested are listed below:

_PCEPI_ - Personal Consumption Expenditures

_UNRATE_ - Unemployment Rate

_MICH_ - University of Michigan: Inflation Expectations

_IPMAN_ - Industrial Production: Manufacturing

_HOUST_ - New Privately-Owned Housing Units Started: Total Units

Why might we use these variables? According to Walras's law everything in the economy is connected in someway; but these variables were not selected at random. Economic theory tell us that certain sectors such as manufacturing may have some relation to inflation, hence the use of _IPMAN_. Other areas of economic theory tell us that consumer expectations and behaviors can drive market movements that can cause inflation. The variable such as _HOUST_ may give us the the ability to say that if consumers are demanding houses, that then there is an increase in new housing units and could suggest that consumer confidence is high.This could also mean that individuals have more disposable income and that they are spending that income on durable goods, which in turn could suggest that unemployment is low. In just that one variable, we can learn a good deal about state of the economy. Variables such as _MICH_ that provide "expectations" are similar in nature to _HOUST_ as it gives insight into what consumers believe to be the state of the economy. In some ways the expectations set by consumers create a self-fulfilling prophecy where the consumers create the environment they expected to experience. 

Before getting into the specifications of the model there are some properties of the data that need to be addressed. First, the data is from FRED (Federal Reserve Economic Data) and starts in January of 1982. Secondly and more importantly the data has been manipulated in order to be able to run a forecast. The variable _MICH_ was not seasonally adjusted and therefore, needed to be difference'd. Differentiating makes the data stationary meaning that over time the the properties of the data such as mean and variance stay constant over time. 

```{r seasonality, include=FALSE}

DATA_MUT <- Xw %>% select(c(PCEPI, UNRATE, dif_MICH, IPMAN, HOUST)) %>%
  mutate(infl = 1200*log(PCEPI/lag(PCEPI))) %>%
  mutate(dinfl = infl - lag(infl,1)) %>%
  mutate(dinfl12 = 100*log(PCEPI/lag(PCEPI,12)) - lag(infl, 12)) %>%
  mutate(unrate = UNRATE - lag(UNRATE)) %>%
  mutate(ipman = 1200*log(IPMAN/lag(IPMAN))) %>%
  mutate(houst = 100*log(HOUST/lag(HOUST))) %>%
    select(-c(PCEPI, UNRATE, IPMAN, HOUST)) %>%
    drop_na()
  
train_data <- DATA_MUT %>% filter_index(~ "2019-12")
test_data <- DATA_MUT %>% filter_index("2020-01" ~ .)

```  

### _Model Specifications_

The equation below is the model specification for the original Phillips curve. 

\[ \pi^{12}_t - \pi_{t-12} = \phi + \beta(B)\Delta_{\pi_{t-12}} + \gamma(B)_{u_{t-12}} + \varepsilon_t \]

The theoretical framework for a steady state economy suggest inflation and unemployment can reach an equilibrium when there are no shock present. Without diving too deep into the math, this specification fulfills this assumption and is necessary for this analysis. Now the question becomes, can we get a better result from a specification that has been around for decades by replacing unemployment with one of the variables listed above. 

```{r melt, include=FALSE}

Zm <- melt(DATA_MUT, "Month")
ggplot(Zm, aes(Month, value )) + 
         geom_line() +
         facet_wrap(~variable, scales = "free", ncol = 2)

```

```{r train}
fitPC <- train_data %>% 
  model(
    mPC = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                 lag(unrate,12) + lag(unrate,13) + lag(unrate,14) +
                 lag(unrate,15) + lag(unrate,16) + lag(unrate,17) +
                 lag(unrate,18) + lag(unrate,19) + lag(unrate,20) +
                 lag(unrate,21) + lag(unrate,22) + lag(unrate,23)),

    mMICH = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                   lag(dif_MICH,12) + lag(dif_MICH,13) + lag(dif_MICH,14) +
                   lag(dif_MICH,15) + lag(dif_MICH,16) + lag(dif_MICH,17) +
                   lag(dif_MICH,18) + lag(dif_MICH,19) + lag(dif_MICH,20) + 
                   lag(dif_MICH,21) + lag(dif_MICH,22) + lag(dif_MICH,23)),

    mIPMAN = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                   lag(ipman,12) + lag(ipman,13) + lag(ipman,14) +
                   lag(ipman,15) + lag(ipman,16) + lag(ipman,17) +
                   lag(ipman,18) + lag(ipman,19) + lag(ipman,20) +
                   lag(ipman,21) + lag(ipman,22) + lag(ipman,23)),

    mHOUST = TSLM(dinfl12 ~ 1 +
                 lag(dinfl,12) + lag(dinfl,13) + lag(dinfl,14) +
                 lag(dinfl,15) + lag(dinfl,16) + lag(dinfl,17) +
                 lag(dinfl,18) + lag(dinfl,19) + lag(dinfl,20) +
                 lag(dinfl,21) + lag(dinfl,22) + lag(dinfl,23) +
                   lag(houst,12) + lag(houst,13) + lag(houst,14) +
                   lag(houst,15) + lag(houst,16) + lag(houst,17) +
                   lag(houst,18) + lag(houst,19) + lag(houst,20) +
                   lag(houst,21) + lag(houst,22) + lag(houst,23))
  )
tidy(fitPC)

fitPC %>% select(mHOUST) %>% gg_tsresiduals()
fitPC %>% select(mPC) %>% gg_tsresiduals()
fitPC %>% select(mMICH) %>% gg_tsresiduals()
fitPC %>% select(mIPMAN) %>% gg_tsresiduals()
```

```{r forecast, include=FALSE}
fc_beta <- fitPC %>% forecast(new_data = test_data)
fc_beta %>% autoplot(filter(DATA_MUT, year(Month) > 2016), level = c(90,95))
```

### _Results of the forecast_

```{r final model, include=FALSE}
FITALL <- fitPC %>% mutate(Ensemble = (mPC + mMICH + mIPMAN + mHOUST)/4)

IN_sampleacc <- accuracy(FITALL)

FC_ALL <- FITALL %>% forecast(new_data = test_data)

OUT_sampleacc <- accuracy(FC_ALL, DATA_MUT)

```

```{r accuracy check, include=FALSE}
accuracy(FC_ALL, DATA_MUT)

```
The table below displays the In-Sample Marginal Absolute Percentage Standard Errors also know as MAPE of each of our models, as well as the ensemble model. MAPE is defined as: 
                                  $MAPE = 100/n \sum^n_{t=1}|(A_t - F_t)/A_t|$ 
where $A_t$ is the actual value in our data and $F_t$ is the forecast value. This is a nice measure of accuracy as it is a ratio depending on the actual value in our data. The absolute value of this ratio is then used so that all the error can be cumulative and not have negative errors offset positive errors.

We can see from our table that for In-Sample accuracy, our model utilizing lagged inflation as well as the Industry Production variable produced the lowest MAPE. However, all of our models do not fit the data well and only slightly improve upon one another. Something to note is that the ensemble model is a close second to _IPMAN_, while the traditional Phillips curve is third. 


```{r Training Output}
IN_SAMPLE <- IN_sampleacc %>% 
  select(c(".model", ".type", "MAPE")) %>%
  kable(format = "html", table.attr = "sytle='widtth:30%$;' ") %>% 
  kableExtra::kable_styling()

IN_SAMPLE
  
```

This next table reports the Out-of-Sample MAPE for each of our models as well as the ensemble model. It is clear that from our forecast, the model containing Industry Production metrics performed the best but not by a large margin. Again, the ensemble model is a close second; however, something to note is that MAPE for _HOUST_ improves, while other variables such as _MICH_ again shows a really poor MAPE. This may be a surprise based on the economic intuition that suggest that individuals expectations can create the economic environment they are expecting.


```{r Test Output}
OUT_SAMPLE <- OUT_sampleacc %>% 
  select(c(".model", ".type", "MAPE")) %>%
  kable(format = "html", table.attr = "sytle='widtth:30%$;' ") %>% 
  kableExtra::kable_styling()

OUT_SAMPLE
  
```

Lastly, We can see from the plot below that both the green ( _IPMAN_ ) and red ( _ensemble_ ) line most closely follow the dark black line (actual data). Obviously we'd expect this given the MAPE values discussed above.

```{r all model forecast}
FC_ALL %>% autoplot(filter(DATA_MUT, year(Month) > 2016), level = c(95))

```

### _Conclusion_

After thorough analysis we can conclude that both Industrial production (manufacturing) and the ensemble model were the most effective in forecasting the rate of inflation. However, all of the models discussed in this paper 